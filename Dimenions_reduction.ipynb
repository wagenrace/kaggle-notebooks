{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Umap on directional vectors\n",
    "\n",
    "In a previous notebook I got a 300 dimensional vector for each text. Here I am going to explore them more.\n",
    "\n",
    "‚è¨ For the dimension reduction I use [U-MAP](https://umap-learn.readthedocs.io/en/latest/)\n",
    "\n",
    "‚ÜóÔ∏è Directional vector will be the difference between to vector\n",
    "\n",
    "üè† lastly nearest neighbors is used to classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-24T07:26:01.384202Z",
     "iopub.status.busy": "2025-07-24T07:26:01.383880Z",
     "iopub.status.idle": "2025-07-24T07:26:01.955936Z",
     "shell.execute_reply": "2025-07-24T07:26:01.955211Z",
     "shell.execute_reply.started": "2025-07-24T07:26:01.384178Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Loading data\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import regex as re\n",
    "from typing import Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "train_tokens = pd.read_csv(\n",
    "    \"/kaggle/input/extracting-features-with-spacy/training_with_tokens.csv\"\n",
    ")\n",
    "training_data_labels = pd.read_csv(\n",
    "    r\"/kaggle/input/fake-or-real-the-impostor-hunt/data/train.csv\"\n",
    ")\n",
    "test_tokens = pd.read_csv(\n",
    "    \"/kaggle/input/extracting-features-with-spacy/test_with_tokens.csv\"\n",
    ")\n",
    "LEN_VECTOR = 300  # Length of the token vectors\n",
    "token_vectors = [f\"avg_token_vector_{i}\" for i in range(LEN_VECTOR)]\n",
    "\n",
    "train_tokens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mark very wrong data\n",
    "\n",
    "Some text are very clearly wrong. This includes empty strings, repeating a phrase three time, or using none latin letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_none_latin_letters(text):\n",
    "    # Search for things that are NOT\n",
    "    # \\p{Latin} Latin letters\n",
    "    # \\s empty spaces\n",
    "    # \\p{S} Symbols\n",
    "    # \\p{P} Punitions\n",
    "    # \\p{N} Numbers\n",
    "    # \\p{Greek} greek letters (boy do scientists love themselves some greek letters)\n",
    "    # \\¬µ for some reason ¬µ is not part of \\p{Greek}? Weird\n",
    "    return len(re.findall(r\"[^\\p{Latin}\\s\\p{S}\\p{P}\\p{N}\\p{Greek}\\¬µ]+\", text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeats_word_three_times(text: str) -> Tuple[bool, list]:\n",
    "    repeating_phrases = re.findall(r\"([^\\w].{4,})\\1\\1\", text.lower())\n",
    "    if len(repeating_phrases) > 0:\n",
    "        return True, repeating_phrases\n",
    "    else:\n",
    "        return False, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset: pd.DataFrame):\n",
    "    dataset.loc[:, \"is_empty\"] = pd.Series([False] * len(dataset), dtype=bool)\n",
    "    dataset.loc[:, \"has_weird_letters\"] = pd.Series([False] * len(dataset), dtype=bool)\n",
    "    dataset.loc[:, \"repeated_word\"] = pd.Series([False] * len(dataset), dtype=object)\n",
    "\n",
    "    for i, row in dataset.iterrows():\n",
    "        text = row[\"text\"]\n",
    "        # Empty strings are fake\n",
    "        if pd.isna(text) or len(text) == 0:\n",
    "            dataset.loc[i, \"is_empty\"] = True\n",
    "            continue\n",
    "        else:\n",
    "            dataset.loc[i, \"is_empty\"] = False\n",
    "\n",
    "        # Did you use weird letters\n",
    "        # If both are the same we continue\n",
    "        count1 = count_none_latin_letters(text)\n",
    "        dataset.loc[i, \"has_weird_letters\"] = count1 > 0\n",
    "\n",
    "        # Repeating words\n",
    "        # If you repeat a word more then 3 and it is the most repeated\n",
    "        repeats_1 = repeats_word_three_times(text)\n",
    "        dataset.loc[i, \"repeated_word\"] = repeats_1[0]\n",
    "\n",
    "\n",
    "preprocess_dataset(train_tokens)\n",
    "preprocess_dataset(test_tokens)\n",
    "train_tokens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Umap\n",
    "\n",
    "We first do a umap directly on the vectors of the text. Note we have now double the data.\n",
    "Here we try to detect fake and real text, not just select one of them.\n",
    "\n",
    "Below we also plot them against the 3 features we found above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "train_x = train_tokens[token_vectors]\n",
    "test_x = test_tokens[token_vectors]\n",
    "\n",
    "embedding = umap.UMAP().fit_transform(pd.concat([test_x, train_x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(\n",
    "    x=embedding[:, 0],\n",
    "    y=embedding[:, 1],\n",
    "    hue=[\"unknown\"] * len(test_tokens) + list(train_tokens.is_real),\n",
    ")\n",
    "ax.set_title(\"UMAP Projection with is_real Highlighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(\n",
    "    x=embedding[:, 0],\n",
    "    y=embedding[:, 1],\n",
    "    hue=list(test_tokens.is_empty) + list(train_tokens.is_empty),\n",
    ")\n",
    "ax.set_title(\"UMAP Projection with is_empty Highlighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(\n",
    "    x=embedding[:, 0],\n",
    "    y=embedding[:, 1],\n",
    "    hue=list(test_tokens.has_weird_letters) + list(train_tokens.has_weird_letters),\n",
    ")\n",
    "ax.set_title(\"UMAP Projection with has_weird_letters Highlighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(\n",
    "    x=embedding[:, 0],\n",
    "    y=embedding[:, 1],\n",
    "    hue=list(test_tokens.repeated_word) + list(train_tokens.repeated_word),\n",
    ")\n",
    "ax.set_title(\"UMAP Projection with Repeated Words Highlighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Umap with filters\n",
    "\n",
    "We can see above that a big separation in the Umap we can also do with our filters. So lets do it and only the the umap for the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filtered = train_tokens.loc[\n",
    "    (train_tokens.is_empty == False)\n",
    "    & (train_tokens.has_weird_letters == False)\n",
    "    & (train_tokens.repeated_word == False)\n",
    "]\n",
    "test_filtered = test_tokens.loc[\n",
    "    (test_tokens.is_empty == False)\n",
    "    & (test_tokens.has_weird_letters == False)\n",
    "    & (test_tokens.repeated_word == False)\n",
    "]\n",
    "\n",
    "train_x2 = train_filtered[token_vectors]\n",
    "test_x2 = test_filtered[token_vectors]\n",
    "\n",
    "train_x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "embedding2 = umap.UMAP().fit_transform(pd.concat([test_x2, train_x2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "ax = sns.scatterplot(\n",
    "    x=embedding2[:, 0],\n",
    "    y=embedding2[:, 1],\n",
    "    hue=[2] * len(test_filtered) + list(train_filtered.is_real),\n",
    "    palette={0: \"red\", 1: \"green\", 2: [0.8, 0.8, 0.8]},  # turquoise\n",
    ")\n",
    "ax.set_title(\n",
    "    \"UMAP Projection without Weird Letters, Empty Strings or Repeated Words Highlighted\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ÜóÔ∏è Difference vector\n",
    "\n",
    "Let create new vectors from text 1 to text 2. This will give us a difference vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_difference_vectors = []\n",
    "is_from_real = []\n",
    "\n",
    "# Testing data\n",
    "article_ids = sorted([int(i) for i in test_tokens.article_id.unique()])\n",
    "num_test_data = len(article_ids)\n",
    "\n",
    "for i in article_ids:\n",
    "    vec1 = test_tokens.loc[(test_tokens.article_id == i) & (test_tokens.file_id == 1)]\n",
    "    vec2 = test_tokens.loc[(test_tokens.article_id == i) & (test_tokens.file_id == 2)]\n",
    "\n",
    "    difference_vector = vec1[token_vectors].values - vec2[token_vectors].values\n",
    "    is_from_real.append(\"unknown\")\n",
    "\n",
    "    all_difference_vectors.append(difference_vector)\n",
    "\n",
    "# Training data\n",
    "article_ids = sorted([int(i) for i in train_tokens.article_id.unique()])\n",
    "num_train_data = len(article_ids)\n",
    "\n",
    "for i in article_ids:\n",
    "    vec1 = train_tokens.loc[\n",
    "        (train_tokens.article_id == i) & (train_tokens.file_id == 1)\n",
    "    ]\n",
    "    vec2 = train_tokens.loc[\n",
    "        (train_tokens.article_id == i) & (train_tokens.file_id == 2)\n",
    "    ]\n",
    "\n",
    "    difference_vector = vec1[token_vectors].values - vec2[token_vectors].values\n",
    "    if vec1.is_real.values[0]:\n",
    "        is_from_real.append(\"real\")\n",
    "    else:\n",
    "        is_from_real.append(\"fake\")\n",
    "\n",
    "    all_difference_vectors.append(difference_vector)\n",
    "\n",
    "# Reshape output\n",
    "all_difference_vectors = np.array(all_difference_vectors)[:, 0, :]\n",
    "is_from_real = np.array(is_from_real)\n",
    "\n",
    "print(all_difference_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "embedding3 = umap.UMAP().fit_transform(all_difference_vectors)\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "\n",
    "ax = sns.scatterplot(\n",
    "    x=embedding3[:, 0],\n",
    "    y=embedding3[:, 1],\n",
    "    hue=is_from_real,\n",
    "    palette={\"fake\": \"red\", \"real\": \"green\", \"unknown\": [0.8, 0.8, 0.8]},\n",
    ")\n",
    "ax.set_title(\n",
    "    \"UMAP Projection of Difference Vectors with Real and Fake Articles Highlighted\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "embedding_train = embedding3[num_test_data:]\n",
    "embedding_test = embedding3[:num_test_data]\n",
    "\n",
    "for n_neighbors in range(2, 20, 2):\n",
    "    nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm=\"ball_tree\").fit(\n",
    "        embedding_train\n",
    "    )\n",
    "\n",
    "    # Training results\n",
    "    distances, indices = nbrs.kneighbors(embedding_train)\n",
    "\n",
    "    all_correct = 0\n",
    "    for indic in indices:\n",
    "        predicted = (\n",
    "            is_from_real[indic[1:] + num_test_data]\n",
    "            == is_from_real[indic[0] + num_test_data]\n",
    "        ).mean()\n",
    "        all_correct += predicted.round()\n",
    "\n",
    "    print(\n",
    "        f\"acc training: {all_correct/num_train_data:.3f} for {nbrs.n_neighbors - 1} neighbors\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbrs_train = NearestNeighbors(n_neighbors=8, algorithm=\"ball_tree\").fit(embedding_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, indices = nbrs_train.kneighbors(embedding_train)\n",
    "for article_id, indic in enumerate(indices):\n",
    "    avg_fake = (is_from_real[indic[1:] + num_test_data] == \"fake\").mean()\n",
    "    uncertainty = np.abs(avg_fake.round() - avg_fake)\n",
    "    real_text_id = avg_fake.round() + 1\n",
    "\n",
    "    training_data_labels.loc[article_id, \"prediction\"] = real_text_id\n",
    "    training_data_labels.loc[article_id, \"uncertainty\"] = uncertainty\n",
    "\n",
    "training_data_labels.loc[:, \"is_correct\"] = (\n",
    "    training_data_labels.prediction == training_data_labels.real_text_id\n",
    ")\n",
    "\n",
    "print(f\"acc: {training_data_labels.is_correct.sum() / len(training_data_labels)}\")\n",
    "\n",
    "training_data_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(\n",
    "    data=training_data_labels, x=\"uncertainty\", hue=\"is_correct\", multiple=\"stack\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbrs = NearestNeighbors(n_neighbors=7, algorithm=\"ball_tree\").fit(embedding_train)\n",
    "distances, indices = nbrs.kneighbors(embedding_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gives a test acc of 0.86307\n",
    "submission = pd.DataFrame(columns=[\"id\", \"real_text_id\"])\n",
    "\n",
    "distances, indices = nbrs.kneighbors(embedding_test)\n",
    "for article_id, indic in enumerate(indices):\n",
    "    real_text_id = (is_from_real[indic + num_test_data] == \"fake\").mean().round() + 1\n",
    "    submission = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame([{\"id\": article_id, \"real_text_id\": int(real_text_id)}]),\n",
    "            submission,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "submission = submission.sort_values(by=\"id\")\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding an extra filter for the submission increases the acc to 0.87136\n",
    "submission_filtered = submission.copy()\n",
    "\n",
    "for i, row in submission_filtered.iterrows():\n",
    "    file1 = test_tokens[(test_tokens.article_id == row.id) & (test_tokens.file_id == 1)]\n",
    "    file2 = test_tokens[(test_tokens.article_id == row.id) & (test_tokens.file_id == 2)]\n",
    "\n",
    "    # Empty is always fake\n",
    "    if file1.is_empty.values[0]:\n",
    "        submission_filtered.loc[submission_filtered.id == row.id, \"real_text_id\"] = 2\n",
    "        continue\n",
    "    elif file2.is_empty.values[0]:\n",
    "        submission_filtered.loc[submission_filtered.id == row.id, \"real_text_id\"] = 1\n",
    "        continue\n",
    "\n",
    "    # Weird letters is always fake\n",
    "    if file1.has_weird_letters.values[0]:\n",
    "        submission_filtered.loc[submission_filtered.id == row.id, \"real_text_id\"] = 2\n",
    "        continue\n",
    "    elif file2.has_weird_letters.values[0]:\n",
    "        submission_filtered.loc[submission_filtered.id == row.id, \"real_text_id\"] = 1\n",
    "        continue\n",
    "\n",
    "    # Repeated words is always fake\n",
    "    if file1.repeated_word.values[0]:\n",
    "        submission_filtered.loc[submission_filtered.id == row.id, \"real_text_id\"] = 2\n",
    "        continue\n",
    "    elif file2.repeated_word.values[0]:\n",
    "        submission_filtered.loc[submission_filtered.id == row.id, \"real_text_id\"] = 1\n",
    "        continue\n",
    "\n",
    "submission_filtered.to_csv(\"submission_filtered.csv\", index=False)\n",
    "submission_filtered.to_csv(\"submission.csv\", index=False)\n",
    "submission_filtered.head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 12964783,
     "sourceId": 105874,
     "sourceType": "competition"
    },
    {
     "sourceId": 252134293,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "kaggle-projects (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
